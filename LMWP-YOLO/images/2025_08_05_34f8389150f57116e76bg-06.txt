Overall Architecture
The module takes a single feature tensor as input and processes it through three parallel attention streams to generate three independently calibrated feature tensors. These are then aggregated to produce the final output. The input tensor is depicted as a cube with dimensions labeled H (Height), W (Width), and C (Channel).

1. Channel Attention Stream (Bottom Path)
This stream calculates and applies channel-wise attention.

Squeeze Transformation: The input tensor is fed into a "Squeeze Transformation" block. This block applies both Average Pooling (AvgPool) and Standard Deviation Pooling (StdPool) across the spatial (H and W) dimensions to capture channel-wise statistics, producing a descriptive feature vector.

Excitation Transformation: The resulting feature vector is passed to an "Excitation Transformation" block. This block uses a 1×K 2D Convolution (1xK Conv2d), which functions like a fully connected layer, to generate channel attention weights.

Weight Application: These weights are passed through a Sigmoid activation function. The resulting values are then applied to the original input tensor (passed through an "Identity" path) using "Broadcast element-wise multiplication" (⊗). This produces a channel-recalibrated feature map.

2. Height Attention Stream (Top Path)
This stream calculates and applies attention along the height dimension.

Permutation: The input tensor undergoes a "Permutation" operation (Permutation ΔH), which reorders its dimensions to treat the Height dimension as the Channel dimension (e.g., from H x W x C to C x W x H).

Attention Calculation: The permuted tensor is processed through a similar Squeeze and Excitation mechanism (as described in the Channel Attention Stream) to compute attention weights for the height dimension.

Weight Application: The computed weights are applied to the permuted tensor via Sigmoid and element-wise multiplication.

Inverse Permutation: The resulting tensor is permuted back to its original dimensional order (H x W x C), yielding a height-recalibrated feature map.

3. Width Attention Stream (Middle Path)
This stream calculates and applies attention along the width dimension, analogous to the height stream.

Permutation: The input tensor is permuted to treat the Width dimension as the Channel dimension (e.g., from H x W x C to H x C x W).

Attention Calculation & Application: The permuted tensor is processed by the Squeeze and Excitation mechanism to compute and apply width-wise attention weights.

Inverse Permutation: The result is permuted back to the original H x W x C order, yielding a width-recalibrated feature map.

4. Integration
The three calibrated feature maps (from the Channel, Height, and Width streams) are aggregated to form the final output.

Each of the three feature maps is scaled by a factor of 1/3.

The scaled feature maps are combined using "Broadcast element-wise summation" (⊕).

The result is a single output feature tensor that has been refined with collaborative attention across all three dimensions.

Legend
⊗: Broadcast element-wise multiplication.

⊕: Broadcast element-wise summation.